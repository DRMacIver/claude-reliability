# Snapshot Testing for Claude Code

This document describes the snapshot testing infrastructure for testing Claude Code behavior by recording and replaying tool call transcripts.

## Overview

Snapshot tests verify Claude Code behavior by:
1. **Recording**: Running Claude Code against a test scenario and capturing the transcript (tool calls and results)
2. **Replaying**: Re-executing the tool calls from the transcript and verifying outputs match
3. **Verification**: Running post-conditions and comparing final directory state

## Why Snapshot Testing?

Snapshot tests allow us to:
- Test complex multi-step workflows without running Claude Code every time
- Verify that tool implementations produce consistent results
- Catch regressions in tool behavior
- Test hook behavior by replaying recorded sessions

## Test Structure

Each test is a directory under `tests/` containing:

```
tests/my-test/
├── setup.py              # Creates test environment (required)
├── story.md              # Test description and prompt (required for recording)
├── transcript.jsonl      # Claude Code native transcript (generated by recording)
├── transcript.md         # Human-readable transcript (auto-generated)
├── directory-snapshot.json  # File hashes for final state verification (auto-generated)
├── post-condition.py     # Optional verification script
└── config.yaml           # Optional test configuration
```

### setup.py

Creates the initial test environment. Run in a temporary git repository.

```python
#!/usr/bin/env python3
"""Set up test environment."""
from pathlib import Path

# Create files needed for the test
Path("src").mkdir()
Path("src/main.py").write_text('print("hello")\n')

print("Setup complete")
```

### story.md

Describes the test scenario and contains the prompt for recording:

```markdown
# My Test

## Scenario
Description of what this test verifies.

## Expected Behavior
1. Agent does X
2. Agent does Y

## Post-Condition
- Verify X happened
- Verify Y happened

## Prompt
```
The prompt to send to Claude Code when recording.
```
```

### post-condition.py

Optional script that verifies the final state. Exit with code 0 for success.

```python
#!/usr/bin/env python3
"""Verify test postconditions."""
import sys
from pathlib import Path

if not Path("output.txt").exists():
    print("FAIL: output.txt not found")
    sys.exit(1)

print("PASS: output.txt exists")
```

### directory-snapshot.json

Auto-generated file containing SHA256 hashes of all files in the final directory state. Used to verify that Write/Edit operations produce byte-wise identical files across replay runs.

## Running Tests

### Replay Mode (Default)

Run recorded tests without Claude Code:

```bash
python -m snapshot_tests.run_snapshots                    # All tests
python -m snapshot_tests.run_snapshots my-test           # Specific test
python -m snapshot_tests.run_snapshots --verbose         # Detailed output
```

### Record Mode

Run Claude Code to capture a new transcript:

```bash
python -m snapshot_tests.run_snapshots --mode=record my-test
```

### Save Directory Snapshot

Bootstrap directory snapshots from successful replay:

```bash
python -m snapshot_tests.run_snapshots --save-snapshot my-test
```

## Tool Simulation

The simulator (`snapshot_tests/simulator.py`) executes tool calls during replay:

### Supported Tools

| Tool | Behavior |
|------|----------|
| Bash | Executes command, normalizes timestamps/timing |
| Read | Reads file, tracks for read-before-write enforcement |
| Write | Writes file, enforces read-before-write for existing files |
| Edit | Edits file, enforces read-before-edit |
| Glob | Finds files matching pattern |
| Grep | Searches file contents |
| Task | Passthrough (skip execution) |
| TodoWrite | Passthrough (skip execution) |
| Skill | Passthrough (skip execution) |
| AskUserQuestion | Passthrough (skip execution) |

### Output Normalization

Bash output is normalized to handle variations:
- `ls -la` timestamps: `"Jan 20 13:19"` → `"<timestamp>"`
- Cargo timing: `"in 0.25s"` → `"in <time>"`
- Cargo binary hashes: `"rust_utils-878eb25838ec6d42"` → `"rust_utils-<hash>"`

### Read-Before-Write Enforcement

The simulator tracks file reads to match Claude Code's behavior:
- Write to existing file requires prior Read
- Edit requires prior Read
- Reads are tracked at session level

## Verification Levels

1. **Tool Execution**: Tool calls must execute without errors
2. **Output Matching**: Tool outputs are compared (mismatches are warnings)
3. **Directory Snapshot**: Final file states must be byte-wise identical
4. **Post-Conditions**: Custom verification scripts must pass

## Known Limitations

### Claude Code Read Tracking

Claude Code's read-before-write enforcement has complex behavior that we don't fully replicate:
- Reads may expire after many tool calls
- The exact reset mechanism is unknown

The directory snapshot comparison ensures that even if intermediate states differ, the final file state is correct.

### Output Mismatches

Some tool outputs will always differ:
- Timestamps and timing information
- File hashes generated by build tools
- Test execution order (tests may run in different order)

These are normalized or marked as non-fatal mismatches.

## Creating a New Test

1. Create test directory:
   ```bash
   mkdir tests/my-new-test
   ```

2. Create `setup.py`:
   ```python
   #!/usr/bin/env python3
   """Set up test environment."""
   from pathlib import Path
   # Create initial state
   ```

3. Create `story.md` with scenario and prompt

4. Record the test:
   ```bash
   python -m snapshot_tests.run_snapshots --mode=record my-new-test
   ```

5. Create `post-condition.py` for verification

6. Save directory snapshot:
   ```bash
   python -m snapshot_tests.run_snapshots --save-snapshot my-new-test
   ```

7. Verify replay works:
   ```bash
   python -m snapshot_tests.run_snapshots my-new-test
   ```

## Architecture

```
snapshot_tests/
├── __init__.py           # Package initialization
├── run_snapshots.py      # Main test runner
├── simulator.py          # Tool call simulator
├── transcript.py         # Transcript parsing
├── compile_transcript.py # Generate human-readable transcript
├── placeholder.py        # Placeholder registry for fuzzy matching
├── commit_tracker.py     # Git SHA normalization
├── plugin_setup.py       # Plugin installation for tests
└── pty_runner.py         # PTY-based Claude Code runner
```

## Troubleshooting

### "File has not been read yet"

The simulator enforces read-before-write. Ensure files are read before editing.

### Directory snapshot mismatch

Write/Edit operations produced different file content. Check:
1. Are edits using the exact same old_string/new_string?
2. Is there any path substitution needed?

### Missing pexpect module

Install dependencies:
```bash
uv pip install -e .
```

### Transcript not found after recording

Claude stores transcripts in `~/.claude/projects/<hash>/<session-id>.jsonl`. The runner should copy it automatically.
